# ğŸ¾ OpenClaw Companion

Your AI, alive. Talk to an animated Live2D avatar through voice or text â€” Android, Web, and beyond. Powered by [OpenClaw](https://github.com/openclaw/openclaw).

<p align="center">
  <img src="preview.jpg" alt="OpenClaw Companion â€” Live2D voice assistant" width="300" />
</p>

## âœ¨ Features

- **Push-to-talk voice** â€” hold, speak, release
- **Streaming sentence-by-sentence TTS** â€” hear the first sentence while the AI is still generating
- **Emotion-reactive avatar** â€” 9 animated emotions (happy, sad, surprised, thinking, confused, laughing, neutral, angry, love)
- **Live2D avatars** â€” 7 animated models with dual display mode (orb / Live2D)
- **Barge-in** â€” interrupt the AI mid-response; partial context is preserved
- **Conversation memory** â€” 10-exchange sliding window, persists across reconnects
- **Smart Listen mode** â€” ambient always-on listening with wake word detection
- **Speaker identification** â€” auto-enrolls voices, recognizes speakers, prioritizes the owner
- **Google Meet bot** â€” joins meetings with Live2D avatar, listens, responds when mentioned
- **Bilingual TTS** â€” auto-detects meeting/conversation language (EN/ES) and responds accordingly
- **Calendar auto-join** â€” automatically joins Google Meet calls from your calendar
- **Vision & file analysis** â€” send images or text files for AI analysis
- **Web search** â€” automatic DuckDuckGo search integration for factual queries
- **Multiple TTS engines** â€” Kokoro (local GPU, ~460ms), Edge TTS (cloud, ~2300ms), XTTS v2 (local GPU, voice cloning)
- **Text chat with markdown** â€” full markdown rendering, code blocks as artifacts
- **Inline buttons** â€” interactive response options from the AI
- **File & image attachments** â€” send photos and documents for analysis
- **Works over Tailscale / LAN / WAN**
- **Headphone media button & lock screen support** (Android)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Android App â”‚â—„â”€â”€â”€â”€ WebSocket (WS/WSS) â”€â”€â”€â”€â–ºâ”‚   Voice Server (Node.js)         â”‚
â”‚  or Web App  â”‚   audio/text/images/files     â”‚   Port 3200 (WS) / 3443 (WSS)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Meet â”‚â—„â”€â”€ Puppeteer + PulseAudio â”€â”€â–ºâ”‚   Meet Bot (Node.js)             â”‚
â”‚  (browser)   â”‚   audio capture/inject       â”‚   Port 3300                      â”‚
â”‚              â”‚â—„â”€â”€ Live2D canvas stream       â”‚   â€¢ Calendar auto-join (ICS)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚   â€¢ Bilingual EN/ES auto-detect  â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   Shared Services                â”‚
                                              â”‚                                  â”‚
                                              â”‚   Whisper ASR (:9000)  â—„â”€â”€ GPU   â”‚
                                              â”‚   Kokoro TTS  (:5004)  â—„â”€â”€ GPU   â”‚
                                              â”‚   XTTS v2     (:5002)  â—„â”€â”€ GPU   â”‚
                                              â”‚   OpenClaw Gateway (:18789)      â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Voice input** â†’ Client records PCM audio (16kHz mono) â†’ encodes WAV â†’ sends base64 over WebSocket
2. **Transcription** â†’ Whisper ASR converts speech to text
3. **Speaker ID** â†’ Resemblyzer identifies who's speaking (auto-enrolls new speakers)
4. **Web search** â†’ If the query needs facts, DuckDuckGo results are injected as context
5. **LLM streaming** â†’ OpenClaw Gateway streams response via SSE (HTTP) or native WebSocket
6. **Sentence splitting** â†’ Response is split at sentence boundaries as tokens arrive
7. **Parallel TTS** â†’ Each sentence is sent to TTS immediately (text + audio sent concurrently)
8. **Client playback** â†’ Audio chunks play sequentially while text appears in real-time

## ğŸš€ Quick Start

### Prerequisites
- **Docker** with Docker Compose v2 (included in Docker Desktop)
- **OpenClaw Gateway** running (locally or remote with valid token)
- **Linux/macOS** or **WSL2** on Windows
- Optional: **NVIDIA GPU** for faster Whisper and local TTS

### 1. Start the Voice Server (Automated)

The easiest way â€” run the interactive setup wizard:

```bash
cd server
chmod +x setup.sh
./setup.sh
```

This will:
- âœ… Check prerequisites (Docker, Docker Compose)
- âœ… Guide you through configuration (language, TTS engine, GPU)
- âœ… Generate `.env` with your settings
- âœ… Pull Docker images
- âœ… Start all services (Whisper ASR + Voice Server)
- âœ… Verify service health
- âœ… Display connection info

**Expected output:**
```
Your Voice Server is ready to connect to OpenClaw Gateway

Connection Information:
  WebSocket URL:    ws://localhost:3200
  Auth Token:       [your-token]
  
Services:
  Voice Server:     http://localhost:3200
  Whisper (STT):    http://localhost:9000
  Kokoro TTS:       http://localhost:5004 (if enabled)
```

### 2. Install & Configure Client

**Android** â€” Build APK with Docker (no SDK needed):
```bash
cd android
docker build -f Dockerfile -t openclaw-companion-builder .
docker cp $(docker create openclaw-companion-builder):/app/app/build/outputs/apk/debug/app-debug.apk ./openclaw-companion.apk
```

**Web** â€” Build static site:
```bash
cd web
npm install && npm run build
# Deploy dist/ to any static host (Netlify, Vercel, etc.)
```

### 3. Connect the Client

Open Settings in the app and enter:
- **Server URL**: `ws://YOUR_SERVER_IP:3200` (or `wss://...` for TLS)
- **Auth Token**: shown by setup.sh (or check: `docker compose logs voice-server | grep Token`)

### Troubleshooting

If setup fails, check the logs:
```bash
cd server
docker compose logs -f                 # All services
docker compose logs -f voice-server    # Just voice server
docker compose logs -f whisper         # Just Whisper ASR
```

For detailed setup instructions, see [**server/README.md**](server/README.md).

## ğŸ¤– Google Meet Bot

An AI participant that joins your Google Meet calls with an animated Live2D avatar.

**Features:**
- Joins meetings as a guest named "Jarvis" (configurable)
- Live2D avatar rendered as camera feed (Mao, Hiyori, or Rice)
- Listens to all participants via PulseAudio virtual devices
- Responds when mentioned by name
- Auto-detects language (EN/ES) via Whisper and responds accordingly
- Bilingual TTS (Kokoro for speed, Edge as fallback)
- Meeting transcript saved as markdown
- Calendar auto-join from Google Calendar ICS feed

**Quick start:**
```bash
cd meet-bot
docker build -t meet-bot .
docker run -d --name meet-bot --network host \
  -e GATEWAY_WS_URL=ws://127.0.0.1:18789 \
  -e GATEWAY_TOKEN=your-token \
  -e BOT_NAME=Jarvis \
  -e LIVE2D_MODEL=Mao \
  meet-bot

# Join a meeting
curl -X POST http://localhost:3300/join \
  -H 'Content-Type: application/json' \
  -d '{"meetLink":"https://meet.google.com/abc-defg-hij"}'
```

**Optional: Calendar auto-join** â€” add `-e GOOGLE_CALENDAR_ICS=<your-private-ics-url>` to automatically join meetings from your Google Calendar.

See [**meet-bot/README.md**](meet-bot/README.md) for full documentation.

## ğŸ“‚ Project Structure

```
openclaw-companion/
â”œâ”€â”€ server/                          Voice bridge server (Node.js + Python)
â”‚   â”œâ”€â”€ setup.sh                     ğŸš€ Interactive setup wizard (START HERE)
â”‚   â”œâ”€â”€ docker-compose.yml           Services definition (Whisper + Voice Server)
â”‚   â”œâ”€â”€ Dockerfile                   Voice server container image
â”‚   â”œâ”€â”€ index.js                     WebSocket server, LLM streaming, TTS
â”‚   â”œâ”€â”€ speaker_service.py           Speaker ID (Resemblyzer) + web search
â”‚   â”œâ”€â”€ start.sh                     Entrypoint (starts Python + Node services)
â”‚   â”œâ”€â”€ package.json                 Node.js dependencies
â”‚   â”œâ”€â”€ .env.example                 Configuration template
â”‚   â””â”€â”€ README.md                    ğŸ“– Detailed server docs & API reference
â”œâ”€â”€ meet-bot/                        Google Meet bot
â”‚   â”œâ”€â”€ Dockerfile                   Container image (Chromium + PulseAudio + Xvfb)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.js                 HTTP API + event wiring
â”‚   â”‚   â”œâ”€â”€ meet-joiner.js           Puppeteer Meet login/join
â”‚   â”‚   â”œâ”€â”€ audio-pipeline.js        PulseAudio capture/inject
â”‚   â”‚   â”œâ”€â”€ transcriber.js           VAD + Whisper ASR
â”‚   â”‚   â”œâ”€â”€ ai-responder.js          Gateway WS + bilingual TTS
â”‚   â”‚   â”œâ”€â”€ live2d-canvas.js         Live2D avatar â†’ WebRTC camera
â”‚   â”‚   â”œâ”€â”€ calendar-sync.js         ICS feed â†’ auto-join scheduler
â”‚   â”‚   â”œâ”€â”€ meeting-memory.js        Transcript storage
â”‚   â”‚   â””â”€â”€ config.js                Environment config
â”‚   â”œâ”€â”€ public/live2d/               Live2D model assets
â”‚   â””â”€â”€ scripts/                     Entrypoint + audio setup
â”œâ”€â”€ android/                         Android app (Kotlin)
â”‚   â”œâ”€â”€ Dockerfile                   Docker-based APK build
â”‚   â”œâ”€â”€ build.gradle                 App configuration
â”‚   â””â”€â”€ README.md                    ğŸ“– Android setup & build guide
â”œâ”€â”€ web/                             Web client (React + TypeScript + Vite)
â”‚   â”œâ”€â”€ vite.config.ts               Build configuration
â”‚   â”œâ”€â”€ src/components/              React components
â”‚   â””â”€â”€ README.md                    ğŸ“– Web client setup & deployment guide
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md              ğŸ“– Full architecture & protocol spec
â”œâ”€â”€ PLAN.md                          ğŸ“– Development roadmap
â””â”€â”€ README.md                        This file
```

## ğŸ“– Documentation

**Quick Navigation:**
- ğŸ‘‰ **Just starting?** â†’ Run `server/setup.sh` (recommended for first-time setup)
- ğŸ”§ **Server configuration?** â†’ See [**server/README.md**](server/README.md)
  - All environment variables
  - WebSocket protocol reference
  - Troubleshooting & health checks
  - Advanced TLS setup
- ğŸ“± **Building the Android app?** â†’ See [**android/README.md**](android/README.md)
- ğŸŒ **Building the web client?** â†’ See [**web/README.md**](web/README.md)
- ğŸ—ï¸ **Want to understand the architecture?** â†’ See [Architecture](#architecture) above

## ğŸ“„ License

[MIT](LICENSE)

## ğŸ”— Links

- [OpenClaw](https://github.com/nichochar/openclaw) â€” the AI gateway this connects to
- [Whisper ASR](https://github.com/ahmetoner/whisper-asr-webservice) â€” speech recognition service
- [Kokoro TTS](https://github.com/remsky/Kokoro-FastAPI) â€” fast local TTS engine
- [XTTS v2](https://github.com/coqui-ai/TTS) â€” voice cloning TTS
- [Edge TTS](https://github.com/rany2/edge-tts) â€” cloud text-to-speech engine
