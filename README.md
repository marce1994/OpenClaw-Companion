# ğŸ¾ OpenClaw Companion

Your AI, alive. Talk to an animated Live2D avatar through voice or text â€” Android, Web, and beyond. Powered by [OpenClaw](https://github.com/openclaw/openclaw).

<p align="center">
  <img src="preview.jpg" alt="OpenClaw Companion â€” Live2D voice assistant" width="300" />
</p>

## âœ¨ Features

- **Push-to-talk voice** â€” hold, speak, release
- **Streaming sentence-by-sentence TTS** â€” hear the first sentence while the AI is still generating
- **Emotion-reactive avatar** â€” 9 animated emotions (happy, sad, surprised, thinking, confused, laughing, neutral, angry, love)
- **Live2D avatars** â€” 7 animated models with dual display mode (orb / Live2D)
- **Barge-in** â€” interrupt the AI mid-response; partial context is preserved
- **Conversation memory** â€” 10-exchange sliding window, persists across reconnects
- **Smart Listen mode** â€” ambient always-on listening with wake word detection
- **Speaker identification** â€” auto-enrolls voices, recognizes speakers, prioritizes the owner
- **Vision & file analysis** â€” send images or text files for AI analysis
- **Web search** â€” automatic DuckDuckGo search integration for factual queries
- **Multiple TTS engines** â€” Kokoro (local GPU, ~460ms), Edge TTS (cloud, ~2300ms), XTTS v2 (local GPU, voice cloning)
- **Text chat with markdown** â€” full markdown rendering, code blocks as artifacts
- **Inline buttons** â€” interactive response options from the AI
- **File & image attachments** â€” send photos and documents for analysis
- **Works over Tailscale / LAN / WAN**
- **Headphone media button & lock screen support** (Android)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Android App â”‚â—„â”€â”€â”€â”€ WebSocket (WS/WSS) â”€â”€â”€â”€â–ºâ”‚   Voice Server (Node.js)         â”‚
â”‚  or Web App  â”‚   audio/text/images/files     â”‚   Port 3200 (WS) / 3443 (WSS)   â”‚
â”‚              â”‚â—„â”€â”€ reply_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                  â”‚
â”‚  â€¢ Voice     â”‚â—„â”€â”€ audio_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â€¢ Avatar    â”‚â—„â”€â”€ buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚ Speaker ID   â”‚ (Python)      â”‚
â”‚  â€¢ Text chat â”‚                              â”‚   â”‚ :3201        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚   â”‚ + Web Search â”‚               â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                                              â”‚   â”‚ Whisper ASR   â”‚ :9000       â”‚
                                              â”‚   â”‚ large-v3-turboâ”‚              â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                                              â”‚   â”‚ OpenClaw Gateway      â”‚      â”‚
                                              â”‚   â”‚ HTTP or WebSocket     â”‚      â”‚
                                              â”‚   â”‚ :18789               â”‚      â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                                              â”‚   â”‚ TTS Engine    â”‚              â”‚
                                              â”‚   â”‚ Kokoro :5004  â”‚              â”‚
                                              â”‚   â”‚ XTTS   :5002  â”‚              â”‚
                                              â”‚   â”‚ Edge (cloud)  â”‚              â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Voice input** â†’ Client records PCM audio (16kHz mono) â†’ encodes WAV â†’ sends base64 over WebSocket
2. **Transcription** â†’ Whisper ASR converts speech to text
3. **Speaker ID** â†’ Resemblyzer identifies who's speaking (auto-enrolls new speakers)
4. **Web search** â†’ If the query needs facts, DuckDuckGo results are injected as context
5. **LLM streaming** â†’ OpenClaw Gateway streams response via SSE (HTTP) or native WebSocket
6. **Sentence splitting** â†’ Response is split at sentence boundaries as tokens arrive
7. **Parallel TTS** â†’ Each sentence is sent to TTS immediately (text + audio sent concurrently)
8. **Client playback** â†’ Audio chunks play sequentially while text appears in real-time

## ğŸš€ Quick Start

### 1. Start Supporting Services

```bash
# Whisper ASR (speech recognition)
docker run -d --name whisper-asr \
  --gpus all \
  -p 9000:9000 \
  -e ASR_MODEL=large-v3-turbo \
  onerahmet/openai-whisper-asr-webservice:latest

# Kokoro TTS (optional, for fast local TTS)
docker run -d --name kokoro-tts \
  --gpus all \
  -p 5004:8080 \
  ghcr.io/remsky/kokoro-fastapi-gpu:v0.4.2
```

### 2. Build & Run Voice Server

```bash
cd server
docker build -t jarvis-voice-img .
docker run -d --name jarvis-voice \
  --network host \
  -e AUTH_TOKEN=my-secret-token \
  -e GATEWAY_TOKEN=your-gateway-token \
  -e TTS_ENGINE=kokoro \
  -v /tmp/speaker-profiles:/data/speakers \
  jarvis-voice-img
```

### 3. Install Client

**Android** â€” Build APK with Docker (no SDK needed):
```bash
cd android
docker build -f Dockerfile -t openclaw-companion-builder .
docker cp $(docker create openclaw-companion-builder):/app/app/build/outputs/apk/debug/app-debug.apk ./openclaw-companion.apk
```

**Web** â€” Build static site:
```bash
cd web
npm install && npm run build
# Deploy dist/ to any static host
```

### 4. Configure Client

Open Settings in the app and enter:
- **Server URL**: `ws://YOUR_SERVER_IP:3200` (or `wss://...` for TLS)
- **Auth Token**: the `AUTH_TOKEN` you set above

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ server/                Voice bridge server (Node.js + Python)
â”‚   â”œâ”€â”€ index.js           WebSocket server, LLM streaming, TTS
â”‚   â”œâ”€â”€ speaker_service.py Speaker ID (Resemblyzer) + web search (DuckDuckGo)
â”‚   â”œâ”€â”€ Dockerfile         Server container build
â”‚   â”œâ”€â”€ start.sh           Entrypoint (starts Python + Node)
â”‚   â””â”€â”€ README.md          Detailed server setup & WebSocket protocol
â”œâ”€â”€ android/               Android app (Kotlin)
â”‚   â”œâ”€â”€ Dockerfile         Docker-based APK build
â”‚   â””â”€â”€ README.md          Android setup guide
â”œâ”€â”€ web/                   Web client (React + TypeScript + Vite)
â”‚   â””â”€â”€ README.md          Web client setup guide
â””â”€â”€ README.md              This file
```

## ğŸ“– Documentation

- **[Server README](server/README.md)** â€” complete server setup, all environment variables, WebSocket protocol reference, troubleshooting
- **[Android README](android/README.md)** â€” Android app build, configuration, features
- **[Web README](web/README.md)** â€” web client build, deployment, TLS setup
- **[Architecture](../docs/ARCHITECTURE.md)** â€” system architecture and WebSocket protocol specification

## ğŸ“„ License

[MIT](LICENSE)

## ğŸ”— Links

- [OpenClaw](https://github.com/nichochar/openclaw) â€” the AI gateway this connects to
- [Whisper ASR](https://github.com/ahmetoner/whisper-asr-webservice) â€” speech recognition service
- [Kokoro TTS](https://github.com/remsky/Kokoro-FastAPI) â€” fast local TTS engine
- [XTTS v2](https://github.com/coqui-ai/TTS) â€” voice cloning TTS
- [Edge TTS](https://github.com/rany2/edge-tts) â€” cloud text-to-speech engine
