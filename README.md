# ğŸ¾ OpenClaw Companion

Your AI, alive. Talk to an animated Live2D avatar through voice or text â€” Android, Web, and beyond. Powered by [OpenClaw](https://github.com/openclaw/openclaw).

<p align="center">
  <img src="preview.jpg" alt="OpenClaw Companion â€” Live2D voice assistant" width="300" />
</p>

## âœ¨ Features

- **Push-to-talk voice** â€” hold, speak, release
- **Streaming sentence-by-sentence TTS** â€” hear the first sentence while the AI is still generating
- **Emotion-reactive avatar** â€” 9 animated emotions (happy, sad, surprised, thinking, confused, laughing, neutral, angry, love)
- **Live2D avatars** â€” 7 animated models with dual display mode (orb / Live2D)
- **Barge-in** â€” interrupt the AI mid-response; partial context is preserved
- **Conversation memory** â€” 10-exchange sliding window, persists across reconnects
- **Smart Listen mode** â€” ambient always-on listening with wake word detection
- **Speaker identification** â€” auto-enrolls voices, recognizes speakers, prioritizes the owner
- **Vision & file analysis** â€” send images or text files for AI analysis
- **Web search** â€” automatic DuckDuckGo search integration for factual queries
- **Multiple TTS engines** â€” Kokoro (local GPU, ~460ms), Edge TTS (cloud, ~2300ms), XTTS v2 (local GPU, voice cloning)
- **Text chat with markdown** â€” full markdown rendering, code blocks as artifacts
- **Inline buttons** â€” interactive response options from the AI
- **File & image attachments** â€” send photos and documents for analysis
- **Works over Tailscale / LAN / WAN**
- **Headphone media button & lock screen support** (Android)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Android App â”‚â—„â”€â”€â”€â”€ WebSocket (WS/WSS) â”€â”€â”€â”€â–ºâ”‚   Voice Server (Node.js)         â”‚
â”‚  or Web App  â”‚   audio/text/images/files     â”‚   Port 3200 (WS) / 3443 (WSS)   â”‚
â”‚              â”‚â—„â”€â”€ reply_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                  â”‚
â”‚  â€¢ Voice     â”‚â—„â”€â”€ audio_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â€¢ Avatar    â”‚â—„â”€â”€ buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚ Speaker ID   â”‚ (Python)      â”‚
â”‚  â€¢ Text chat â”‚                              â”‚   â”‚ :3201        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚   â”‚ + Web Search â”‚               â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                                              â”‚   â”‚ Whisper ASR   â”‚ :9000       â”‚
                                              â”‚   â”‚ large-v3-turboâ”‚              â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                                              â”‚   â”‚ OpenClaw Gateway      â”‚      â”‚
                                              â”‚   â”‚ HTTP or WebSocket     â”‚      â”‚
                                              â”‚   â”‚ :18789               â”‚      â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                              â”‚          â”‚                       â”‚
                                              â”‚          â–¼                       â”‚
                                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                                              â”‚   â”‚ TTS Engine    â”‚              â”‚
                                              â”‚   â”‚ Kokoro :5004  â”‚              â”‚
                                              â”‚   â”‚ XTTS   :5002  â”‚              â”‚
                                              â”‚   â”‚ Edge (cloud)  â”‚              â”‚
                                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Voice input** â†’ Client records PCM audio (16kHz mono) â†’ encodes WAV â†’ sends base64 over WebSocket
2. **Transcription** â†’ Whisper ASR converts speech to text
3. **Speaker ID** â†’ Resemblyzer identifies who's speaking (auto-enrolls new speakers)
4. **Web search** â†’ If the query needs facts, DuckDuckGo results are injected as context
5. **LLM streaming** â†’ OpenClaw Gateway streams response via SSE (HTTP) or native WebSocket
6. **Sentence splitting** â†’ Response is split at sentence boundaries as tokens arrive
7. **Parallel TTS** â†’ Each sentence is sent to TTS immediately (text + audio sent concurrently)
8. **Client playback** â†’ Audio chunks play sequentially while text appears in real-time

## ğŸš€ Quick Start

### Prerequisites
- **Docker** with Docker Compose v2 (included in Docker Desktop)
- **OpenClaw Gateway** running (locally or remote with valid token)
- **Linux/macOS** or **WSL2** on Windows
- Optional: **NVIDIA GPU** for faster Whisper and local TTS

### 1. Start the Voice Server (Automated)

The easiest way â€” run the interactive setup wizard:

```bash
cd server
chmod +x setup.sh
./setup.sh
```

This will:
- âœ… Check prerequisites (Docker, Docker Compose)
- âœ… Guide you through configuration (language, TTS engine, GPU)
- âœ… Generate `.env` with your settings
- âœ… Pull Docker images
- âœ… Start all services (Whisper ASR + Voice Server)
- âœ… Verify service health
- âœ… Display connection info

**Expected output:**
```
Your Voice Server is ready to connect to OpenClaw Gateway

Connection Information:
  WebSocket URL:    ws://localhost:3200
  Auth Token:       [your-token]
  
Services:
  Voice Server:     http://localhost:3200
  Whisper (STT):    http://localhost:9000
  Kokoro TTS:       http://localhost:5004 (if enabled)
```

### 2. Install & Configure Client

**Android** â€” Build APK with Docker (no SDK needed):
```bash
cd android
docker build -f Dockerfile -t openclaw-companion-builder .
docker cp $(docker create openclaw-companion-builder):/app/app/build/outputs/apk/debug/app-debug.apk ./openclaw-companion.apk
```

**Web** â€” Build static site:
```bash
cd web
npm install && npm run build
# Deploy dist/ to any static host (Netlify, Vercel, etc.)
```

### 3. Connect the Client

Open Settings in the app and enter:
- **Server URL**: `ws://YOUR_SERVER_IP:3200` (or `wss://...` for TLS)
- **Auth Token**: shown by setup.sh (or check: `docker compose logs voice-server | grep Token`)

### Troubleshooting

If setup fails, check the logs:
```bash
cd server
docker compose logs -f                 # All services
docker compose logs -f voice-server    # Just voice server
docker compose logs -f whisper         # Just Whisper ASR
```

For detailed setup instructions, see [**server/README.md**](server/README.md).

## ğŸ“‚ Project Structure

```
openclaw-companion/
â”œâ”€â”€ server/                          Voice bridge server (Node.js + Python)
â”‚   â”œâ”€â”€ setup.sh                     ğŸš€ Interactive setup wizard (START HERE)
â”‚   â”œâ”€â”€ docker-compose.yml           Services definition (Whisper + Voice Server)
â”‚   â”œâ”€â”€ Dockerfile                   Voice server container image
â”‚   â”œâ”€â”€ index.js                     WebSocket server, LLM streaming, TTS
â”‚   â”œâ”€â”€ speaker_service.py           Speaker ID (Resemblyzer) + web search
â”‚   â”œâ”€â”€ start.sh                     Entrypoint (starts Python + Node services)
â”‚   â”œâ”€â”€ package.json                 Node.js dependencies
â”‚   â”œâ”€â”€ .env.example                 Configuration template
â”‚   â””â”€â”€ README.md                    ğŸ“– Detailed server docs & API reference
â”œâ”€â”€ android/                         Android app (Kotlin + JetpackCompose)
â”‚   â”œâ”€â”€ Dockerfile                   Docker-based APK build
â”‚   â”œâ”€â”€ build.gradle                 App configuration
â”‚   â””â”€â”€ README.md                    ğŸ“– Android setup & build guide
â”œâ”€â”€ web/                             Web client (React + TypeScript + Vite)
â”‚   â”œâ”€â”€ vite.config.ts               Build configuration
â”‚   â”œâ”€â”€ src/components               React components
â”‚   â””â”€â”€ README.md                    ğŸ“– Web client setup & deployment guide
â””â”€â”€ README.md                        This file
```

## ğŸ“– Documentation

**Quick Navigation:**
- ğŸ‘‰ **Just starting?** â†’ Run `server/setup.sh` (recommended for first-time setup)
- ğŸ”§ **Server configuration?** â†’ See [**server/README.md**](server/README.md)
  - All environment variables
  - WebSocket protocol reference
  - Troubleshooting & health checks
  - Advanced TLS setup
- ğŸ“± **Building the Android app?** â†’ See [**android/README.md**](android/README.md)
- ğŸŒ **Building the web client?** â†’ See [**web/README.md**](web/README.md)
- ğŸ—ï¸ **Want to understand the architecture?** â†’ See [Architecture](#architecture) above

## ğŸ“„ License

[MIT](LICENSE)

## ğŸ”— Links

- [OpenClaw](https://github.com/nichochar/openclaw) â€” the AI gateway this connects to
- [Whisper ASR](https://github.com/ahmetoner/whisper-asr-webservice) â€” speech recognition service
- [Kokoro TTS](https://github.com/remsky/Kokoro-FastAPI) â€” fast local TTS engine
- [XTTS v2](https://github.com/coqui-ai/TTS) â€” voice cloning TTS
- [Edge TTS](https://github.com/rany2/edge-tts) â€” cloud text-to-speech engine
