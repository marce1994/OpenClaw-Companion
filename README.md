# ğŸ¾ OpenClaw Companion

Your AI, alive. Talk to an animated Live2D avatar through voice or text â€” Android, Web, and Google Meet. Powered by [OpenClaw](https://github.com/openclaw/openclaw).

<p align="center">
  <img src="preview.jpg" alt="OpenClaw Companion â€” Live2D voice assistant" width="300" />
</p>

## âœ¨ Features

- **Push-to-talk voice** â€” hold, speak, release
- **Streaming TTS** â€” hear the first sentence while the AI is still thinking
- **Emotion-reactive Live2D avatars** â€” 9 emotions, 7 animated models, emoji bubble reactions
- **Smart Listen mode** â€” ambient always-on listening with wake word detection
- **Auto noise detection** â€” quiet/noisy profiles with hysteresis for car mode
- **Speaker identification** â€” auto-enrolls voices, recognizes who's speaking
- **Google Meet bot** â€” joins calls with Live2D avatar, speaker detection, transcript batching, meeting memory
- **Bilingual support** â€” auto-detects language (EN/ES), filters phantom language detections
- **Vision & file analysis** â€” send images or documents for AI analysis
- **Multiple TTS engines** â€” Kokoro (local GPU, ~460ms), Edge TTS (cloud, free)
- **Text chat with markdown** â€” code blocks, inline buttons, artifacts
- **Device capabilities** â€” system info, GPS, camera, Bluetooth car mic via Android bridge
- **Gateway WS integration** â€” native WebSocket protocol v3 with streaming + image attachments
- **Custom whisper-fast server** â€” minimal Python wrapper replacing Speaches' FastAPI (~239ms GPU)
- **Works over Tailscale / LAN / WAN**

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Android App â”‚â—„â”€â”€â”€â”€ WebSocket (WS/WSS) â”€â”€â”€â”€â–ºâ”‚   Voice Server (Node.js)         â”‚
â”‚  or Web App  â”‚   audio/text/images           â”‚   Port 3200 (WS) / 3443 (WSS)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚   + Speaker ID (Python :3201)    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Meet â”‚â—„â”€â”€ Puppeteer + PulseAudio â”€â”€â–ºâ”‚   Meet Bot (Node.js)             â”‚
â”‚  (browser)   â”‚   audio capture/inject       â”‚   Port 3300 (optional)           â”‚
â”‚              â”‚â—„â”€â”€ Live2D canvas stream       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   Shared Services                â”‚
                                              â”‚                                  â”‚
                                              â”‚   whisper-fast   (:9000)  â—„â”€ GPU  â”‚
                                              â”‚   Kokoro TTS    (:5004)  â—„â”€ GPU  â”‚
                                              â”‚   Diarizer      (:3202)  â—„â”€ GPU  â”‚
                                              â”‚   OpenClaw Gateway               â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Voice input** â†’ Client records PCM audio â†’ encodes WAV â†’ sends base64 over WebSocket
2. **Transcription** â†’ Speaches (faster-whisper) converts speech to text
3. **Speaker ID** â†’ Resemblyzer identifies who's speaking
4. **LLM streaming** â†’ OpenClaw Gateway streams response via WebSocket
5. **Sentence splitting** â†’ Response split at sentence boundaries as tokens arrive
6. **Parallel TTS** â†’ Each sentence sent to Kokoro/Edge TTS immediately
7. **Client playback** â†’ Audio chunks play sequentially while text appears in real-time

## ğŸš€ Quick Start

### Prerequisites

- **Docker** with Docker Compose v2
- **OpenClaw Gateway** running ([setup guide](https://github.com/openclaw/openclaw))
- Optional: **NVIDIA GPU** for faster STT and local TTS

### Option 1: Interactive Setup (Recommended)

```bash
git clone https://github.com/openclaw/OpenClaw-Companion.git
cd OpenClaw-Companion
chmod +x setup.sh
./setup.sh
```

The wizard will guide you through configuration, detect GPU, generate `.env`, and start services.

### Option 2: Manual Setup

```bash
git clone https://github.com/openclaw/OpenClaw-Companion.git
cd OpenClaw-Companion

# Configure
cp .env.example .env
nano .env  # Set GATEWAY_TOKEN, AUTH_TOKEN, etc.

# GPU mode (default)
docker compose up -d

# OR CPU-only mode
docker compose -f docker-compose.cpu.yml up -d
```

### Verify It's Running

```bash
docker compose ps                    # Check service status
docker compose logs -f voice-server  # Voice server logs
curl http://localhost:3200/health    # Health check
```

### Connect a Client

- **Android app**: Enter `ws://YOUR_SERVER_IP:3200` and your auth token in Settings
- **Web app**: `cd web && npm install && npm run dev`

## ğŸ“¦ Services

| Service | Port | Description | Required |
|---------|------|-------------|----------|
| `voice-server` | 3200/3443 | WebSocket bridge, TTS, speaker ID | âœ… Yes |
| `whisper-fast` | 9000 | Speech-to-text (custom minimal server + faster-whisper-large-v3-turbo) | âœ… Yes |
| `kokoro-tts` | 5004 | Text-to-speech (GPU, ~330ms) | âœ… Yes (or use Edge TTS) |
| `meet-bot` | 3300 | Google Meet bot with Live2D | Optional |
| `diarizer` | 3202 | Speaker diarization (pyannote) | Optional |

Enable optional services with Docker Compose profiles:

```bash
docker compose --profile meet up -d        # Enable Meet bot
docker compose --profile diarizer up -d    # Enable diarizer
```

## ğŸ¤– Google Meet Bot

Joins your Google Meet calls as a participant with an animated Live2D avatar.

```bash
# Enable and start
docker compose --profile meet up -d

# Join a meeting
curl -X POST http://localhost:3300/join \
  -H 'Content-Type: application/json' \
  -d '{"meetLink":"https://meet.google.com/abc-defg-hij"}'
```

Features: Live2D avatar as camera, bilingual EN/ES, calendar auto-join. See [meet-bot/README.md](meet-bot/README.md).

## ğŸ“‚ Project Structure

```
OpenClaw-Companion/
â”œâ”€â”€ server/                   Voice server (Node.js + Python speaker ID)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ index.js              WebSocket server, LLM streaming, TTS
â”‚   â”œâ”€â”€ speaker_service.py    Speaker ID (Resemblyzer) + web search
â”‚   â””â”€â”€ start.sh              Entrypoint (starts Python + Node)
â”œâ”€â”€ meet-bot/                 Google Meet bot (Node.js + Puppeteer)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ src/                  Meet joiner, audio pipeline, Live2D
â”œâ”€â”€ diarizer/                 Speaker diarization service (Python)
â”œâ”€â”€ android/                  Android app (Kotlin + Live2D)
â”œâ”€â”€ web/                      Web client (React + TypeScript + Vite)
â”œâ”€â”€ docker-compose.yml        GPU services (default)
â”œâ”€â”€ docker-compose.cpu.yml    CPU-only services
â”œâ”€â”€ setup.sh                  Interactive setup wizard
â””â”€â”€ .env.example              Configuration template
```

## âš™ï¸ Configuration

All configuration is via environment variables in `.env`. See [`.env.example`](.env.example) for the full list with descriptions.

Key variables:

| Variable | Description |
|----------|-------------|
| `GATEWAY_WS_URL` | OpenClaw Gateway WebSocket URL |
| `GATEWAY_TOKEN` | Gateway authentication token |
| `AUTH_TOKEN` | Client â†” server shared secret |
| `TTS_ENGINE` | `kokoro` (GPU) or `edge` (cloud) |
| `BOT_NAME` | Bot name / wake word |

## ğŸ”§ Troubleshooting

**Services won't start?**
```bash
docker compose logs -f          # Check all logs
docker compose ps               # Check status
```

**Whisper is slow?**
- Make sure you're using the GPU compose file
- Check GPU allocation: `nvidia-smi`

**No audio response?**
- Verify Kokoro is running: `curl http://localhost:5004/health`
- Try Edge TTS fallback: set `TTS_ENGINE=edge` in `.env`

**Can't connect from Android?**
- Use your machine's LAN IP, not `localhost`
- Check firewall allows port 3200
- For remote access, use Tailscale or set up WSS with TLS

## ğŸ“– More Documentation

- [Architecture & Protocol](docs/ARCHITECTURE.md) â€” full WebSocket protocol spec
- [Server README](server/README.md) â€” detailed server configuration
- [Meet Bot README](meet-bot/README.md) â€” Meet bot setup
- [Android README](android/README.md) â€” building the Android app
- [Web README](web/README.md) â€” web client setup

## ğŸ“„ License

[MIT](LICENSE)
