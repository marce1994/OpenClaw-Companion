###############################################################################
# OpenClaw Companion â€” Docker Compose
#
# Quick start:
#   cp .env.example .env   # edit with your values
#   docker compose up -d   # starts voice-server + whisper + kokoro
#
# Meet bot:  docker compose --profile meet up -d
# All:       docker compose --profile meet up -d
#
# GPU mode:  Set STT_DEVICE=cuda and KOKORO_TAG=latest-gpu in .env
# Rebuild:   docker compose build && docker compose up -d
###############################################################################

services:
  # â”€â”€â”€ Voice Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Core service: bridges Android/Web app â†” OpenClaw Gateway
  # Handles STT routing, TTS streaming, speaker identification
  voice-server:
    build: ./server
    container_name: openclaw-voice-server
    restart: unless-stopped
    network_mode: host
    environment:
      - AUTH_TOKEN=${AUTH_TOKEN:-change-me}
      - WHISPER_URL=http://127.0.0.1:${STT_PORT:-9000}/v1/audio/transcriptions
      - WHISPER_MODEL=${WHISPER_MODEL:-Systran/faster-whisper-large-v3-turbo}
      - TTS_ENGINE=${TTS_ENGINE:-edge}
      - TTS_VOICE=${TTS_VOICE:-es-AR-TomasNeural}
      - BOT_NAME=${BOT_NAME:-assistant}
      - OWNER_NAME=${OWNER_NAME:-User}
      - KOKORO_URL=http://127.0.0.1:${KOKORO_PORT:-5004}
      - KOKORO_VOICE=${KOKORO_VOICE:-em_alex}
      - XTTS_URL=http://127.0.0.1:${XTTS_PORT:-5002}
      - GATEWAY_WS_URL=${GATEWAY_WS_URL:-ws://127.0.0.1:18789}
      - GATEWAY_TOKEN=${GATEWAY_TOKEN:?Set GATEWAY_TOKEN in .env}
      - USE_GATEWAY_WS=${USE_GATEWAY_WS:-true}
      - GW_SESSION_KEY=${VOICE_SESSION_KEY:-voice}
      - PORT=${VOICE_PORT:-3200}
      - WSS_PORT=${WSS_PORT:-3443}
      - TLS_CERT=${TLS_CERT:-}
      - TLS_KEY=${TLS_KEY:-}
      - GOOGLE_CALENDAR_ICS=${GOOGLE_CALENDAR_ICS:-}
      - MAX_MEETINGS=${MAX_MEETINGS:-5}
      - DOCKER_SOCKET=/var/run/docker.sock
    volumes:
      - speaker-data:/data/speakers
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/meetings:/tmp/meetings
    depends_on:
      whisper-fast:
        condition: service_started

  # â”€â”€â”€ Whisper Fast (faster-whisper minimal server) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Minimal HTTP server wrapping faster-whisper. No FastAPI/Gradio overhead.
  # GPU: ~239ms, CPU: ~2-3s per utterance.
  # Model cached locally; HF_HUB_OFFLINE=1 avoids network calls.
  whisper-fast:
    image: ghcr.io/speaches-ai/speaches:${STT_TAG:-latest-cuda}
    container_name: whisper-fast
    restart: unless-stopped
    command: ["python3", "/app/whisper-server.py"]
    ports:
      - "${STT_PORT:-9000}:9000"
    environment:
      - MODEL=${WHISPER_MODEL:-Systran/faster-whisper-large-v3-turbo}
      - DEVICE=${STT_DEVICE:-cuda}
      - COMPUTE_TYPE=${STT_COMPUTE_TYPE:-int8}
      - PORT=9000
      - HF_HUB_OFFLINE=1
      - PYTHONUNBUFFERED=1
    volumes:
      - ${HF_CACHE_PATH:-whisper-models}:/home/ubuntu/.cache/huggingface
      - ./whisper-server/server.py:/app/whisper-server.py:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # â”€â”€â”€ Kokoro TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Fast local TTS. GPU: ~330ms, CPU: ~1s. OpenAI-compatible API.
  # Only needed if TTS_ENGINE=kokoro.
  kokoro-tts:
    image: ghcr.io/remsky/kokoro-fastapi:${KOKORO_TAG:-latest-gpu}
    container_name: kokoro-fastapi
    restart: unless-stopped
    ports:
      - "${KOKORO_PORT:-5004}:8880"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Meet Bot Workers (managed by voice-server orchestrator)
  # ğŸ“ Note: meet-bot:v6 image must be built locally before starting voice-server
  # Workers are created/destroyed dynamically via Docker API
  # Build with: docker build -t meet-bot:v6 ./meet-bot

volumes:
  speaker-data:
  whisper-models:
