###############################################################################
# OpenClaw Companion — Docker Compose
#
# Quick start (Edge TTS, CPU Whisper):
#   cp .env.example .env   # edit with your values
#   docker compose up -d
#
# GPU Whisper (much faster):
#   docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
#
# With Kokoro TTS (local GPU, ~400ms, no voice cloning):
#   Set TTS_ENGINE=kokoro in .env
#   docker compose --profile kokoro up -d
#
# With XTTS v2 (local GPU, voice cloning, ~1s):
#   Set TTS_ENGINE=xtts in .env
#   docker compose --profile xtts up -d
###############################################################################

services:
  # ─── Voice Server (WebSocket bridge) ──────────────────────────────────────
  voice-server:
    build: ./server
    container_name: openclaw-companion-server
    restart: unless-stopped
    ports:
      - "${PORT:-3200}:3200"
    environment:
      - AUTH_TOKEN=${AUTH_TOKEN:-change-me}
      - WHISPER_URL=http://speaches-stt:8000/v1/audio/transcriptions
      - WHISPER_MODEL=${ASR_MODEL:-Systran/faster-whisper-large-v3-turbo}
      - GATEWAY_URL=${GATEWAY_URL:?Set GATEWAY_URL to your OpenClaw chat completions endpoint}
      - GATEWAY_TOKEN=${GATEWAY_TOKEN:?Set GATEWAY_TOKEN to your OpenClaw gateway token}
      - TTS_ENGINE=${TTS_ENGINE:-edge}
      - TTS_VOICE=${TTS_VOICE:-es-AR-TomasNeural}
      - BOT_NAME=${BOT_NAME:-assistant}
      - OWNER_NAME=${OWNER_NAME:-User}
      - KOKORO_URL=${KOKORO_URL:-http://kokoro-tts:8080}
      - KOKORO_VOICE=${KOKORO_VOICE:-em_alex}
      - XTTS_URL=${XTTS_URL:-http://xtts-server:80}
    volumes:
      - speaker-profiles:/data/speakers
    depends_on:
      speaches-stt:
        condition: service_started

  # ─── Speaches STT (faster-whisper, CTranslate2) ────────────────────────────
  # CPU by default. For GPU, see docker-compose.gpu.yml override.
  # Systran model requires HF_TOKEN (set in .env). Without it, falls back to
  # deepdml/faster-whisper-large-v3-turbo-ct2 (ungated, same model weights).
  speaches-stt:
    image: ghcr.io/speaches-ai/speaches:${STT_IMAGE_TAG:-latest-cuda}
    container_name: openclaw-companion-stt
    restart: unless-stopped
    ports:
      - "${STT_PORT:-9000}:8000"
    environment:
      - WHISPER__MODEL=${ASR_MODEL:-Systran/faster-whisper-large-v3-turbo}
      - WHISPER__INFERENCE_DEVICE=${STT_DEVICE:-cuda}
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - whisper-models:/home/ubuntu/.cache/huggingface

  # ─── Kokoro FastAPI TTS (GPU, OpenAI-compatible, ~330ms) ───────────────────
  # Enable: set TTS_ENGINE=kokoro in .env, then:
  #   docker compose --profile kokoro up -d
  kokoro-tts:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
    container_name: openclaw-companion-kokoro
    restart: unless-stopped
    profiles: ["kokoro"]
    ports:
      - "5004:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ─── XTTS v2 (optional, local GPU, voice cloning, ~1s first chunk) ────────
  # Enable: set TTS_ENGINE=xtts in .env, then:
  #   docker compose --profile xtts up -d
  # NOTE: ~30GB image, first pull takes a while
  xtts-server:
    image: ghcr.io/coqui-ai/xtts-streaming-server:latest-cuda121
    container_name: openclaw-companion-xtts
    restart: unless-stopped
    profiles: ["xtts"]
    ports:
      - "5002:80"
    environment:
      - COQUI_TOS_AGREED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  speaker-profiles:
  whisper-models:
